{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Import Pre-Trained Word2Vec Model</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in google word2vec pre-trained model\n",
    "# https://code.google.com/archive/p/word2vec/\n",
    "#model = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in glove pre-trained model and convert to word2vec representation\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = './data/glove.6B/glove.6B.300d.txt'\n",
    "# word2vec_output_file = './data/glove.6B/glove.6B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "model = KeyedVectors.load_word2vec_format('glove.6B.300d.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Explore Word2Vec Similarity</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as',\n",
       " 'it',\n",
       " 'by',\n",
       " 'at',\n",
       " '(',\n",
       " ')',\n",
       " 'from',\n",
       " 'his',\n",
       " \"''\",\n",
       " '``',\n",
       " 'an',\n",
       " 'be',\n",
       " 'has',\n",
       " 'are',\n",
       " 'have',\n",
       " 'but',\n",
       " 'were',\n",
       " 'not',\n",
       " 'this',\n",
       " 'who',\n",
       " 'they',\n",
       " 'had',\n",
       " 'i',\n",
       " 'which',\n",
       " 'will',\n",
       " 'their',\n",
       " ':',\n",
       " 'or',\n",
       " 'its',\n",
       " 'one',\n",
       " 'after',\n",
       " 'new',\n",
       " 'been',\n",
       " 'also',\n",
       " 'we',\n",
       " 'would',\n",
       " 'two',\n",
       " 'more',\n",
       " \"'\",\n",
       " 'first',\n",
       " 'about',\n",
       " 'up',\n",
       " 'when',\n",
       " 'year',\n",
       " 'there',\n",
       " 'all',\n",
       " '--',\n",
       " 'out',\n",
       " 'she',\n",
       " 'other',\n",
       " 'people',\n",
       " \"n't\",\n",
       " 'her',\n",
       " 'percent',\n",
       " 'than',\n",
       " 'over',\n",
       " 'into',\n",
       " 'last',\n",
       " 'some',\n",
       " 'government',\n",
       " 'time',\n",
       " '$',\n",
       " 'you',\n",
       " 'years',\n",
       " 'if',\n",
       " 'no',\n",
       " 'world',\n",
       " 'can',\n",
       " 'three',\n",
       " 'do',\n",
       " ';',\n",
       " 'president',\n",
       " 'only',\n",
       " 'state',\n",
       " 'million',\n",
       " 'could',\n",
       " 'us',\n",
       " 'most',\n",
       " '_',\n",
       " 'against',\n",
       " 'u.s.',\n",
       " 'so',\n",
       " 'them',\n",
       " 'what',\n",
       " 'him',\n",
       " 'united',\n",
       " 'during',\n",
       " 'before',\n",
       " 'may',\n",
       " 'since',\n",
       " 'many',\n",
       " 'while',\n",
       " 'where',\n",
       " 'states',\n",
       " 'because',\n",
       " 'now',\n",
       " 'city',\n",
       " 'made',\n",
       " 'like',\n",
       " 'between',\n",
       " 'did',\n",
       " 'just',\n",
       " 'national',\n",
       " 'day',\n",
       " 'country',\n",
       " 'under',\n",
       " 'such',\n",
       " 'second',\n",
       " 'then',\n",
       " 'company',\n",
       " 'group',\n",
       " 'any',\n",
       " 'through',\n",
       " 'china',\n",
       " 'four',\n",
       " 'being',\n",
       " 'down',\n",
       " 'war',\n",
       " 'back',\n",
       " 'off',\n",
       " 'south',\n",
       " 'american',\n",
       " 'minister',\n",
       " 'police',\n",
       " 'well',\n",
       " 'including',\n",
       " 'team',\n",
       " 'international',\n",
       " 'week',\n",
       " 'officials',\n",
       " 'still',\n",
       " 'both',\n",
       " 'even',\n",
       " 'high',\n",
       " 'part',\n",
       " 'told',\n",
       " 'those',\n",
       " 'end',\n",
       " 'former',\n",
       " 'these',\n",
       " 'make',\n",
       " 'billion',\n",
       " 'work',\n",
       " 'our',\n",
       " 'home',\n",
       " 'school',\n",
       " 'party',\n",
       " 'house',\n",
       " 'old',\n",
       " 'later',\n",
       " 'get',\n",
       " 'another',\n",
       " 'tuesday',\n",
       " 'news',\n",
       " 'long',\n",
       " 'five',\n",
       " 'called',\n",
       " '1',\n",
       " 'wednesday',\n",
       " 'military',\n",
       " 'way',\n",
       " 'used',\n",
       " 'much',\n",
       " 'next',\n",
       " 'monday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'game',\n",
       " 'here',\n",
       " '?',\n",
       " 'should',\n",
       " 'take',\n",
       " 'very',\n",
       " 'my',\n",
       " 'north',\n",
       " 'security',\n",
       " 'season',\n",
       " 'york',\n",
       " 'how',\n",
       " 'public',\n",
       " 'early',\n",
       " 'according',\n",
       " 'several',\n",
       " 'court',\n",
       " 'say',\n",
       " 'around',\n",
       " 'foreign',\n",
       " '10',\n",
       " 'until',\n",
       " 'set',\n",
       " 'political',\n",
       " 'says',\n",
       " 'market',\n",
       " 'however',\n",
       " 'family',\n",
       " 'life',\n",
       " 'same',\n",
       " 'general',\n",
       " '–',\n",
       " 'left',\n",
       " 'good',\n",
       " 'top',\n",
       " 'university',\n",
       " 'going',\n",
       " 'number',\n",
       " 'major',\n",
       " 'known',\n",
       " 'points',\n",
       " 'won',\n",
       " 'six',\n",
       " 'month',\n",
       " 'dollars',\n",
       " 'bank',\n",
       " '2',\n",
       " 'iraq',\n",
       " 'use',\n",
       " 'members',\n",
       " 'each',\n",
       " 'area',\n",
       " 'found',\n",
       " 'official',\n",
       " 'sunday',\n",
       " 'place',\n",
       " 'go',\n",
       " 'based',\n",
       " 'among',\n",
       " 'third',\n",
       " 'times',\n",
       " 'took',\n",
       " 'right',\n",
       " 'days',\n",
       " 'local',\n",
       " 'economic',\n",
       " 'countries',\n",
       " 'see',\n",
       " 'best',\n",
       " 'report',\n",
       " 'killed',\n",
       " 'held',\n",
       " 'business',\n",
       " 'west',\n",
       " 'does',\n",
       " 'own',\n",
       " '%',\n",
       " 'came',\n",
       " 'law',\n",
       " 'months',\n",
       " 'women',\n",
       " \"'re\",\n",
       " 'power',\n",
       " 'think',\n",
       " 'service',\n",
       " 'children',\n",
       " 'bush',\n",
       " 'show',\n",
       " '/',\n",
       " 'help',\n",
       " 'chief',\n",
       " 'saturday',\n",
       " 'system',\n",
       " 'john',\n",
       " 'support',\n",
       " 'series',\n",
       " 'play',\n",
       " 'office',\n",
       " 'following',\n",
       " 'me',\n",
       " 'meeting',\n",
       " 'expected',\n",
       " 'late',\n",
       " 'washington',\n",
       " 'games',\n",
       " 'european',\n",
       " 'league',\n",
       " 'reported',\n",
       " 'final',\n",
       " 'added',\n",
       " 'without',\n",
       " 'british',\n",
       " 'white',\n",
       " 'history',\n",
       " 'man',\n",
       " 'men',\n",
       " 'became',\n",
       " 'want',\n",
       " 'march',\n",
       " 'case',\n",
       " 'few',\n",
       " 'run',\n",
       " 'money',\n",
       " 'began',\n",
       " 'open',\n",
       " 'name',\n",
       " 'trade',\n",
       " 'center',\n",
       " '3',\n",
       " 'israel',\n",
       " 'oil',\n",
       " 'too',\n",
       " 'al',\n",
       " 'film',\n",
       " 'win',\n",
       " 'led',\n",
       " 'east',\n",
       " 'central',\n",
       " '20',\n",
       " 'air',\n",
       " 'come',\n",
       " 'chinese',\n",
       " 'town',\n",
       " 'leader',\n",
       " 'army',\n",
       " 'line',\n",
       " 'never',\n",
       " 'little',\n",
       " 'played',\n",
       " 'prime',\n",
       " 'death',\n",
       " 'companies',\n",
       " 'least',\n",
       " 'put',\n",
       " 'forces',\n",
       " 'past',\n",
       " 'de',\n",
       " 'half',\n",
       " 'june',\n",
       " 'saying',\n",
       " 'know',\n",
       " 'federal',\n",
       " 'french',\n",
       " 'peace',\n",
       " 'earlier',\n",
       " 'capital',\n",
       " 'force',\n",
       " 'great',\n",
       " 'union',\n",
       " 'near',\n",
       " 'released',\n",
       " 'small',\n",
       " 'department',\n",
       " 'every',\n",
       " 'health',\n",
       " 'japan',\n",
       " 'head',\n",
       " 'ago',\n",
       " 'night',\n",
       " 'big',\n",
       " 'cup',\n",
       " 'election',\n",
       " 'region',\n",
       " 'director',\n",
       " 'talks',\n",
       " 'program',\n",
       " 'far',\n",
       " 'today',\n",
       " 'statement',\n",
       " 'july',\n",
       " 'although',\n",
       " 'district',\n",
       " 'again',\n",
       " 'born',\n",
       " 'development',\n",
       " 'leaders',\n",
       " 'council',\n",
       " 'close',\n",
       " 'record',\n",
       " 'along',\n",
       " 'county',\n",
       " 'france',\n",
       " 'went',\n",
       " 'point',\n",
       " 'must',\n",
       " 'spokesman',\n",
       " 'your',\n",
       " 'member',\n",
       " 'plan',\n",
       " 'financial',\n",
       " 'april',\n",
       " 'recent',\n",
       " 'campaign',\n",
       " 'become',\n",
       " 'troops',\n",
       " 'whether',\n",
       " 'lost',\n",
       " 'music',\n",
       " '15',\n",
       " 'got',\n",
       " 'israeli',\n",
       " '30',\n",
       " 'need',\n",
       " '4',\n",
       " 'lead',\n",
       " 'already',\n",
       " 'russia',\n",
       " 'though',\n",
       " 'might',\n",
       " 'free',\n",
       " 'hit',\n",
       " 'rights',\n",
       " '11',\n",
       " 'information',\n",
       " 'away',\n",
       " '12',\n",
       " '5',\n",
       " 'others',\n",
       " 'control',\n",
       " 'within',\n",
       " 'large',\n",
       " 'economy',\n",
       " 'press',\n",
       " 'agency',\n",
       " 'water',\n",
       " 'died',\n",
       " 'career',\n",
       " 'making',\n",
       " '...',\n",
       " 'deal',\n",
       " 'attack',\n",
       " 'side',\n",
       " 'seven',\n",
       " 'better',\n",
       " 'less',\n",
       " 'september',\n",
       " 'once',\n",
       " 'clinton',\n",
       " 'main',\n",
       " 'due',\n",
       " 'committee',\n",
       " 'building',\n",
       " 'conference',\n",
       " 'club',\n",
       " 'january',\n",
       " 'decision',\n",
       " 'stock',\n",
       " 'america',\n",
       " 'given',\n",
       " 'give',\n",
       " 'often',\n",
       " 'announced',\n",
       " 'television',\n",
       " 'industry',\n",
       " 'order',\n",
       " 'young',\n",
       " \"'ve\",\n",
       " 'palestinian',\n",
       " 'age',\n",
       " 'start',\n",
       " 'administration',\n",
       " 'russian',\n",
       " 'prices',\n",
       " 'round',\n",
       " 'december',\n",
       " 'nations',\n",
       " \"'m\",\n",
       " 'human',\n",
       " 'india',\n",
       " 'defense',\n",
       " 'asked',\n",
       " 'total',\n",
       " 'october',\n",
       " 'players',\n",
       " 'bill',\n",
       " 'important',\n",
       " 'southern',\n",
       " 'move',\n",
       " 'fire',\n",
       " 'population',\n",
       " 'rose',\n",
       " 'november',\n",
       " 'include',\n",
       " 'further',\n",
       " 'nuclear',\n",
       " 'street',\n",
       " 'taken',\n",
       " 'media',\n",
       " 'different',\n",
       " 'issue',\n",
       " 'received',\n",
       " 'secretary',\n",
       " 'return',\n",
       " 'college',\n",
       " 'working',\n",
       " 'community',\n",
       " 'eight',\n",
       " 'groups',\n",
       " 'despite',\n",
       " 'level',\n",
       " 'largest',\n",
       " 'whose',\n",
       " 'attacks',\n",
       " 'germany',\n",
       " 'august',\n",
       " 'change',\n",
       " 'church',\n",
       " 'nation',\n",
       " 'german',\n",
       " 'station',\n",
       " 'london',\n",
       " 'weeks',\n",
       " 'having',\n",
       " '18',\n",
       " 'research',\n",
       " 'black',\n",
       " 'services',\n",
       " 'story',\n",
       " '6',\n",
       " 'europe',\n",
       " 'sales',\n",
       " 'policy',\n",
       " 'visit',\n",
       " 'northern',\n",
       " 'lot',\n",
       " 'across',\n",
       " 'per',\n",
       " 'current',\n",
       " 'board',\n",
       " 'football',\n",
       " 'ministry',\n",
       " 'workers',\n",
       " 'vote',\n",
       " 'book',\n",
       " 'fell',\n",
       " 'seen',\n",
       " 'role',\n",
       " 'students',\n",
       " 'shares',\n",
       " 'iran',\n",
       " 'process',\n",
       " 'agreement',\n",
       " 'quarter',\n",
       " 'full',\n",
       " 'match',\n",
       " 'started',\n",
       " 'growth',\n",
       " 'yet',\n",
       " 'moved',\n",
       " 'possible',\n",
       " 'western',\n",
       " 'special',\n",
       " '100',\n",
       " 'plans',\n",
       " 'interest',\n",
       " 'behind',\n",
       " 'strong',\n",
       " 'england',\n",
       " 'named',\n",
       " 'food',\n",
       " 'period',\n",
       " 'real',\n",
       " 'authorities',\n",
       " 'car',\n",
       " 'term',\n",
       " 'rate',\n",
       " 'race',\n",
       " 'nearly',\n",
       " 'korea',\n",
       " 'enough',\n",
       " 'site',\n",
       " 'opposition',\n",
       " 'keep',\n",
       " '25',\n",
       " 'call',\n",
       " 'future',\n",
       " 'taking',\n",
       " 'island',\n",
       " '2008',\n",
       " '2006',\n",
       " 'road',\n",
       " 'outside',\n",
       " 'really',\n",
       " 'century',\n",
       " 'democratic',\n",
       " 'almost',\n",
       " 'single',\n",
       " 'share',\n",
       " 'leading',\n",
       " 'trying',\n",
       " 'find',\n",
       " 'album',\n",
       " 'senior',\n",
       " 'minutes',\n",
       " 'together',\n",
       " 'congress',\n",
       " 'index',\n",
       " 'australia',\n",
       " 'results',\n",
       " 'hard',\n",
       " 'hours',\n",
       " 'land',\n",
       " 'action',\n",
       " 'higher',\n",
       " 'field',\n",
       " 'cut',\n",
       " 'coach',\n",
       " 'elections',\n",
       " 'san',\n",
       " 'issues',\n",
       " 'executive',\n",
       " 'february',\n",
       " 'production',\n",
       " 'areas',\n",
       " 'river',\n",
       " 'face',\n",
       " 'using',\n",
       " 'japanese',\n",
       " 'province',\n",
       " 'park',\n",
       " 'price',\n",
       " 'commission',\n",
       " 'california',\n",
       " 'father',\n",
       " 'son',\n",
       " 'education',\n",
       " '7',\n",
       " 'village',\n",
       " 'energy',\n",
       " 'shot',\n",
       " 'short',\n",
       " 'africa',\n",
       " 'key',\n",
       " 'red',\n",
       " 'association',\n",
       " 'average',\n",
       " 'pay',\n",
       " 'exchange',\n",
       " 'eu',\n",
       " 'something',\n",
       " 'gave',\n",
       " 'likely',\n",
       " 'player',\n",
       " 'george',\n",
       " '2007',\n",
       " 'victory',\n",
       " '8',\n",
       " 'low',\n",
       " 'things',\n",
       " '2010',\n",
       " 'pakistan',\n",
       " '14',\n",
       " 'post',\n",
       " 'social',\n",
       " 'continue',\n",
       " 'ever',\n",
       " 'look',\n",
       " 'chairman',\n",
       " 'job',\n",
       " '2000',\n",
       " 'soldiers',\n",
       " 'able',\n",
       " 'parliament',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'problems',\n",
       " 'private',\n",
       " 'lower',\n",
       " 'list',\n",
       " 'built',\n",
       " '13',\n",
       " 'efforts',\n",
       " 'dollar',\n",
       " 'miles',\n",
       " 'included',\n",
       " 'radio',\n",
       " 'live',\n",
       " 'form',\n",
       " 'david',\n",
       " 'african',\n",
       " 'increase',\n",
       " 'reports',\n",
       " 'sent',\n",
       " 'fourth',\n",
       " 'always',\n",
       " 'king',\n",
       " '50',\n",
       " 'tax',\n",
       " 'taiwan',\n",
       " 'britain',\n",
       " '16',\n",
       " 'playing',\n",
       " 'title',\n",
       " 'middle',\n",
       " 'meet',\n",
       " 'global',\n",
       " 'wife',\n",
       " '2009',\n",
       " 'position',\n",
       " 'located',\n",
       " 'clear',\n",
       " 'ahead',\n",
       " '2004',\n",
       " '2005',\n",
       " 'iraqi',\n",
       " 'english',\n",
       " 'result',\n",
       " 'release',\n",
       " 'violence',\n",
       " 'goal',\n",
       " 'project',\n",
       " 'closed',\n",
       " 'border',\n",
       " 'body',\n",
       " 'soon',\n",
       " 'crisis',\n",
       " 'division',\n",
       " '&amp;',\n",
       " 'served',\n",
       " 'tour',\n",
       " 'hospital',\n",
       " 'kong',\n",
       " 'test',\n",
       " 'hong',\n",
       " 'u.n.',\n",
       " 'inc.',\n",
       " 'technology',\n",
       " 'believe',\n",
       " 'organization',\n",
       " 'published',\n",
       " 'weapons',\n",
       " 'agreed',\n",
       " 'why',\n",
       " 'nine',\n",
       " 'summer',\n",
       " 'wanted',\n",
       " 'republican',\n",
       " 'act',\n",
       " 'recently',\n",
       " 'texas',\n",
       " 'course',\n",
       " 'problem',\n",
       " 'senate',\n",
       " 'medical',\n",
       " 'un',\n",
       " 'done',\n",
       " 'reached',\n",
       " 'star',\n",
       " 'continued',\n",
       " 'investors',\n",
       " 'living',\n",
       " 'care',\n",
       " 'signed',\n",
       " '17',\n",
       " 'art',\n",
       " 'provide',\n",
       " 'worked',\n",
       " 'presidential',\n",
       " 'gold',\n",
       " 'obama',\n",
       " 'morning',\n",
       " 'dead',\n",
       " 'opened',\n",
       " \"'ll\",\n",
       " 'event',\n",
       " 'previous',\n",
       " 'cost',\n",
       " 'instead',\n",
       " 'canada',\n",
       " 'band',\n",
       " 'teams',\n",
       " 'daily',\n",
       " '2001',\n",
       " 'available',\n",
       " 'drug',\n",
       " 'coming',\n",
       " '2003',\n",
       " 'investment',\n",
       " '’s',\n",
       " 'michael',\n",
       " 'civil',\n",
       " 'woman',\n",
       " 'training',\n",
       " 'appeared',\n",
       " '9',\n",
       " 'involved',\n",
       " 'indian',\n",
       " 'similar',\n",
       " 'situation',\n",
       " '24',\n",
       " 'los',\n",
       " 'running',\n",
       " 'fighting',\n",
       " 'mark',\n",
       " '40',\n",
       " 'trial',\n",
       " 'hold',\n",
       " 'australian',\n",
       " 'thought',\n",
       " '!',\n",
       " 'study',\n",
       " 'fall',\n",
       " 'mother',\n",
       " 'met',\n",
       " 'relations',\n",
       " 'anti',\n",
       " '2002',\n",
       " 'song',\n",
       " 'popular',\n",
       " 'base',\n",
       " 'tv',\n",
       " 'ground',\n",
       " 'markets',\n",
       " 'ii',\n",
       " 'newspaper',\n",
       " 'staff',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'hope',\n",
       " 'operations',\n",
       " 'pressure',\n",
       " 'americans',\n",
       " 'eastern',\n",
       " 'st.',\n",
       " 'legal',\n",
       " 'asia',\n",
       " 'budget',\n",
       " 'returned',\n",
       " 'considered',\n",
       " 'love',\n",
       " 'wrote',\n",
       " 'stop',\n",
       " 'fight',\n",
       " 'currently',\n",
       " 'charges',\n",
       " 'try',\n",
       " 'aid',\n",
       " 'ended',\n",
       " 'management',\n",
       " 'brought',\n",
       " 'cases',\n",
       " 'decided',\n",
       " 'failed',\n",
       " 'network',\n",
       " 'works',\n",
       " 'gas',\n",
       " 'turned',\n",
       " 'fact',\n",
       " 'vice',\n",
       " 'ca',\n",
       " 'mexico',\n",
       " 'trading',\n",
       " 'especially',\n",
       " 'reporters',\n",
       " 'afghanistan',\n",
       " 'common',\n",
       " 'looking',\n",
       " 'space',\n",
       " 'rates',\n",
       " 'manager',\n",
       " 'loss',\n",
       " '2011',\n",
       " 'justice',\n",
       " 'thousands',\n",
       " 'james',\n",
       " 'rather',\n",
       " 'fund',\n",
       " 'thing',\n",
       " 'republic',\n",
       " 'opening',\n",
       " 'accused',\n",
       " 'winning',\n",
       " 'scored',\n",
       " 'championship',\n",
       " 'example',\n",
       " 'getting',\n",
       " 'biggest',\n",
       " 'performance',\n",
       " 'sports',\n",
       " '1998',\n",
       " 'let',\n",
       " 'allowed',\n",
       " 'schools',\n",
       " 'means',\n",
       " 'turn',\n",
       " 'leave',\n",
       " 'no.',\n",
       " 'robert',\n",
       " 'personal',\n",
       " 'stocks',\n",
       " 'showed',\n",
       " 'light',\n",
       " 'arrested',\n",
       " 'person',\n",
       " 'either',\n",
       " 'offer',\n",
       " 'majority',\n",
       " 'battle',\n",
       " '19',\n",
       " 'class',\n",
       " 'evidence',\n",
       " 'makes',\n",
       " 'society',\n",
       " 'products',\n",
       " 'regional',\n",
       " 'needed',\n",
       " 'stage',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'families',\n",
       " 'construction',\n",
       " 'various',\n",
       " '1996',\n",
       " 'sold',\n",
       " 'independent',\n",
       " 'kind',\n",
       " 'airport',\n",
       " 'paul',\n",
       " 'judge',\n",
       " 'internet',\n",
       " 'movement',\n",
       " 'room',\n",
       " 'followed',\n",
       " 'original',\n",
       " 'angeles',\n",
       " 'italy',\n",
       " '`',\n",
       " 'data',\n",
       " 'comes',\n",
       " 'parties',\n",
       " 'nothing',\n",
       " 'sea',\n",
       " 'bring',\n",
       " '2012',\n",
       " 'annual',\n",
       " 'officer',\n",
       " 'beijing',\n",
       " 'present',\n",
       " 'remain',\n",
       " 'nato',\n",
       " '1999',\n",
       " '22',\n",
       " 'remains',\n",
       " 'allow',\n",
       " 'florida',\n",
       " 'computer',\n",
       " '21',\n",
       " 'contract',\n",
       " 'coast',\n",
       " 'created',\n",
       " 'demand',\n",
       " 'operation',\n",
       " 'events',\n",
       " 'islamic',\n",
       " 'beat',\n",
       " 'analysts',\n",
       " 'interview',\n",
       " 'helped',\n",
       " 'child',\n",
       " 'probably',\n",
       " 'spent',\n",
       " 'asian',\n",
       " 'effort',\n",
       " 'cooperation',\n",
       " 'shows',\n",
       " 'calls',\n",
       " 'investigation',\n",
       " 'lives',\n",
       " 'video',\n",
       " 'yen',\n",
       " 'runs',\n",
       " 'tried',\n",
       " 'bad',\n",
       " 'described',\n",
       " '1994',\n",
       " 'toward',\n",
       " 'written',\n",
       " 'throughout',\n",
       " 'established',\n",
       " 'mission',\n",
       " 'associated',\n",
       " 'buy',\n",
       " 'growing',\n",
       " 'green',\n",
       " 'forward',\n",
       " 'competition',\n",
       " 'poor',\n",
       " 'latest',\n",
       " 'banks',\n",
       " 'question',\n",
       " '1997',\n",
       " 'prison',\n",
       " 'feel',\n",
       " 'attention',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary words\n",
    "model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1043e-01,  8.1217e-01,  7.3668e-02,  1.9023e-01, -5.2888e-02,\n",
       "        6.1468e-02,  1.6076e-01,  4.1302e-01, -3.0199e-01, -9.0827e-01,\n",
       "        2.7504e-01, -3.1890e-02, -2.8842e-01,  2.3447e-01,  4.7679e-01,\n",
       "        5.0124e-01,  2.9371e-01,  2.7029e-01,  5.4745e-02,  9.8038e-02,\n",
       "        5.7116e-01,  3.6755e-01,  4.0734e-02,  3.4347e-01, -1.8256e-01,\n",
       "       -2.8935e-01,  2.3826e-02, -1.9401e-01,  2.4444e-01,  1.3407e-01,\n",
       "       -1.6494e-01, -2.6983e-01, -2.6234e-01, -2.1779e-01, -8.7528e-01,\n",
       "        7.3822e-01, -8.7931e-02, -1.0876e-02, -2.6540e-01,  3.4668e-01,\n",
       "       -5.5814e-01,  1.7591e-01,  1.6926e-01, -1.5725e-01, -5.0430e-01,\n",
       "       -2.0100e-01,  6.6701e-01, -3.2518e-02,  4.5012e-02,  6.5675e-02,\n",
       "       -1.6061e-01, -7.3363e-01,  2.4642e-01,  3.4325e-01,  2.1899e-01,\n",
       "        4.8646e-02, -5.9987e-01, -5.8153e-02, -5.1694e-02, -5.7846e-01,\n",
       "        3.0000e-01,  3.5078e-01,  4.6646e-01, -7.5309e-03,  1.0455e-01,\n",
       "       -5.1016e-01, -5.5987e-02, -1.0295e-01, -2.6476e-01, -4.1230e-02,\n",
       "       -2.8371e-02,  5.1979e-01, -3.4849e-01, -4.7217e-01, -3.7229e-01,\n",
       "       -3.2790e-02,  1.3989e-01,  3.5716e-01,  1.9305e-01, -2.1986e-01,\n",
       "        2.4136e-01,  4.0976e-01,  3.7516e-01,  1.4255e-01, -3.4143e-02,\n",
       "       -7.2653e-01, -1.0832e-01,  6.8616e-01, -2.6335e-01, -4.2345e-01,\n",
       "       -2.4253e-01,  1.5778e-01,  1.4258e-01, -3.2749e-01, -3.4699e-01,\n",
       "        1.6148e-01,  1.9603e-01,  4.1639e-01, -2.3370e-01,  7.5816e-02,\n",
       "        1.5899e-01,  1.6623e-03, -4.8301e-02, -1.0611e-01, -1.9326e-01,\n",
       "        1.4494e-01,  1.5406e-02,  1.0629e-01, -3.6699e-02,  6.3230e-01,\n",
       "        1.2986e-01,  4.9902e-01, -1.1323e+00, -1.2636e-01,  6.4718e-02,\n",
       "        1.2374e-01, -4.9712e-01, -1.4836e-02,  1.0488e-01, -4.9818e-01,\n",
       "       -2.8856e-01,  3.8949e-01, -3.1828e-02, -2.8625e-01, -9.8758e-02,\n",
       "       -7.6990e-02, -2.4234e-01,  7.5793e-01,  3.4835e-01, -7.1030e-01,\n",
       "        4.5318e-01, -3.4418e-01, -1.9459e-01,  6.1478e-01, -2.9010e-02,\n",
       "       -2.7864e-01,  3.8556e-01,  1.0072e-01,  1.2895e-01,  1.7992e-02,\n",
       "        3.3670e-01,  2.0698e-01, -3.8049e-01, -6.6661e-03,  1.1540e-01,\n",
       "       -8.5268e-02, -1.4608e-01,  4.4514e-01, -9.3674e-02,  2.3639e-01,\n",
       "       -1.1447e-01,  1.0948e+00, -5.7823e-02, -1.6295e-01,  5.5880e-01,\n",
       "       -1.8988e-02, -7.1374e-02,  2.1319e-01,  6.1277e-02,  7.2759e-01,\n",
       "        6.2747e-01, -1.9280e-01,  1.3057e-01,  1.7426e-01, -1.0229e-01,\n",
       "        1.5232e-01,  5.2500e-01, -2.1919e-01, -2.7185e-01, -5.4186e-01,\n",
       "        3.1752e-01,  1.6375e-01, -2.9039e-01,  1.7074e-01, -3.1814e-01,\n",
       "       -9.6421e-01, -1.1610e-01, -2.9951e-01,  1.8686e-01, -4.5986e-01,\n",
       "        4.1633e-01, -1.7583e-01, -3.4583e-01, -2.7244e-01, -5.0216e-01,\n",
       "        1.2852e-02,  5.9838e-01, -1.1237e-01,  2.4697e-01, -4.9048e-01,\n",
       "       -4.4188e-01, -1.6255e-01, -7.3313e-01, -3.7677e-01, -6.8925e-01,\n",
       "        6.1174e-02, -4.2101e-01, -1.3153e-01, -8.3590e-03, -1.8360e-02,\n",
       "        1.3686e+00,  4.6169e-02,  9.4622e-01, -1.5126e-02, -1.2477e-01,\n",
       "        4.8754e-01,  2.2384e-01, -2.1820e-01, -2.3389e-01,  1.5207e-01,\n",
       "       -2.8718e-01, -6.3908e-01, -2.2383e-01, -1.8014e-01, -3.3548e-01,\n",
       "        5.3587e-01, -2.9367e-01,  1.0866e-01,  6.3411e-02, -9.3424e-03,\n",
       "       -1.5886e-01,  2.2602e-01,  1.1925e-01, -4.1442e-01, -7.8062e-02,\n",
       "       -9.7857e-02,  2.7938e-01, -1.8348e-01, -3.4584e-01,  1.8489e-01,\n",
       "        1.7402e-01, -5.2198e-01, -4.3306e-01,  1.6256e-01,  1.4032e-01,\n",
       "        3.5124e-01, -1.8280e-01, -3.5984e-01, -1.3009e-01,  1.6304e-01,\n",
       "        3.1734e-01,  3.7716e-03, -4.5498e-02, -4.2066e-01, -4.4419e-01,\n",
       "       -6.8985e-01, -4.9359e-01,  7.0281e-02, -1.4377e-01,  6.2508e-01,\n",
       "       -5.6311e-02,  1.8850e-01, -5.6785e-02,  1.4052e-01,  1.1973e+00,\n",
       "        7.1894e-01,  5.4332e-01, -1.2461e-01, -1.1978e-01,  3.0163e-01,\n",
       "       -1.6273e-01, -4.6740e-02, -2.5249e-01, -3.0659e-02, -3.2271e-01,\n",
       "        3.2361e-01,  3.3244e-01, -2.7819e-02, -3.3367e-01, -2.3444e-02,\n",
       "       -5.0394e-01, -2.0587e-01, -1.3013e-01, -3.5884e-01,  4.5384e-02,\n",
       "       -1.1863e-01, -1.7257e+00,  3.9441e-01, -5.3179e-01,  5.8209e-01,\n",
       "       -6.5771e-01,  3.6849e-01,  2.3518e-01,  1.0802e-01, -8.3159e-01,\n",
       "        6.1486e-01,  2.5547e-01, -4.5289e-01,  5.1446e-01, -1.7911e-01,\n",
       "       -1.2389e-01,  1.8688e-01, -4.1102e-01, -7.0877e-01, -3.7501e-01,\n",
       "       -6.6152e-01,  6.7730e-01,  3.3936e-01,  5.7994e-01,  6.8149e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.get_vector('dog')\n",
    "model['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.788855791091919),\n",
       " ('cat', 0.6816746592521667),\n",
       " ('pet', 0.6291598081588745),\n",
       " ('puppy', 0.5936061143875122),\n",
       " ('hound', 0.5468214750289917),\n",
       " ('horse', 0.5369751453399658),\n",
       " ('animal', 0.5316445827484131),\n",
       " ('cats', 0.5080744028091431),\n",
       " ('canine', 0.5038435459136963),\n",
       " ('pets', 0.501996636390686)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62082785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('man','boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'egg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['banana', 'apple', 'grapes', 'egg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42819834"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('boy','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22413146"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('single','dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Read in Movie Review Data</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cv106_18379.txt</td>\n",
       "      <td>in the line of duty is the critically praised ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>cv446_12209.txt</td>\n",
       "      <td>here is a movie that sadly follows the hong ko...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>cv509_17354.txt</td>\n",
       "      <td>synopsis : nice girl susanne has sex with her ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>cv810_13660.txt</td>\n",
       "      <td>i admit it . \\ni thought arnold schwarzenegger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>cv691_5090.txt</td>\n",
       "      <td>this is the movie that could single-handedly b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>cv226_2618.txt</td>\n",
       "      <td>i had a chance to see a sneak preview of city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>cv768_12709.txt</td>\n",
       "      <td>this is one of the worst big-screen film exper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>cv576_15688.txt</td>\n",
       "      <td>so much for sweet returns . \\nafter smart horr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cv063_28852.txt</td>\n",
       "      <td>would you believe -- in real life , i mean -- ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>cv291_26635.txt</td>\n",
       "      <td>reflecting on \" bedazzled , \" a lively comedy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file                                               text  class\n",
       "106  cv106_18379.txt  in the line of duty is the critically praised ...      0\n",
       "446  cv446_12209.txt  here is a movie that sadly follows the hong ko...      0\n",
       "509  cv509_17354.txt  synopsis : nice girl susanne has sex with her ...      0\n",
       "810  cv810_13660.txt  i admit it . \\ni thought arnold schwarzenegger...      0\n",
       "691   cv691_5090.txt  this is the movie that could single-handedly b...      0\n",
       "226   cv226_2618.txt  i had a chance to see a sneak preview of city ...      1\n",
       "768  cv768_12709.txt  this is one of the worst big-screen film exper...      0\n",
       "576  cv576_15688.txt  so much for sweet returns . \\nafter smart horr...      0\n",
       "63   cv063_28852.txt  would you believe -- in real life , i mean -- ...      0\n",
       "291  cv291_26635.txt  reflecting on \" bedazzled , \" a lively comedy ...      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data into a pandas dataframe\n",
    "import os\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "dfneg = data2df('MoviePosNeg/neg/', 0) # NEG\n",
    "dfpos = data2df('MoviePosNeg/pos/', 1) # POS\n",
    "\n",
    "df = pd.concat([dfpos, dfneg], axis=0)\n",
    "df.sample(frac=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Split Data into Train and Test</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data\n",
    "X, y = df['text'], df['class']\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Setup Preprocessing and Tfidf Vectorization</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a custom preprocessor\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess(text):\n",
    "    # replace one or more white-space characters with a space\n",
    "    regex = re.compile(r\"\\s+\")                               \n",
    "    text = regex.sub(' ', text)    \n",
    "    # lower case\n",
    "    text = text.lower()          \n",
    "    # remove digits and punctuation\n",
    "    regex = re.compile(r\"[%s%s]\" % (string.punctuation, string.digits))\n",
    "    text = regex.sub(' ', text)           \n",
    "    # remove stop words\n",
    "    sw = stopwords.words('english')\n",
    "    text = text.split()                                              \n",
    "    text = ' '.join([w for w in text if w not in sw]) \n",
    "    # remove short words\n",
    "    ' '.join([w for w in text.split() if len(w) >= 2])\n",
    "    # lemmatize\n",
    "    text = ' '.join([(WordNetLemmatizer()).lemmatize(w) for w in text.split()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(preprocess)\n",
    "# df[['text', 'class']].to_csv('./data/ppMoviePosNeg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abraham</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youth</th>\n",
       "      <th>zane</th>\n",
       "      <th>zany</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abandoned  ability  able  aboard  abraham  absence  absent  \\\n",
       "0      0.0        0.0      0.0   0.0     0.0   0.0000      0.0     0.0   \n",
       "1      0.0        0.0      0.0   0.0     0.0   0.2166      0.0     0.0   \n",
       "2      0.0        0.0      0.0   0.0     0.0   0.0000      0.0     0.0   \n",
       "3      0.0        0.0      0.0   0.0     0.0   0.0000      0.0     0.0   \n",
       "4      0.0        0.0      0.0   0.0     0.0   0.0000      0.0     0.0   \n",
       "\n",
       "   absolute  absolutely  ...    younger  youngster  youth  zane  zany  \\\n",
       "0       0.0         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "1       0.0         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "2       0.0         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "3       0.0         0.0  ...   0.088526        0.0    0.0   0.0   0.0   \n",
       "4       0.0         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "\n",
       "   zellweger  zero  zeta  zombie  zone  \n",
       "0        0.0   0.0   0.0     0.0   0.0  \n",
       "1        0.0   0.0   0.0     0.0   0.0  \n",
       "2        0.0   0.0   0.0     0.0   0.0  \n",
       "3        0.0   0.0   0.0     0.0   0.0  \n",
       "4        0.0   0.0   0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(\n",
    "    preprocessor=preprocess,\n",
    "    use_idf=True, smooth_idf=True, norm='l2',\n",
    "    min_df=3, max_df=1.0, max_features=5000, \n",
    "    ngram_range=(1, 1))\n",
    "XTtrain = pd.DataFrame(tv.fit_transform(Xtrain).toarray(), columns=tv.get_feature_names())\n",
    "XTtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Preprocess + Tfidf + Logistic Reg</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\herma\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mdl = LogisticRegression()\n",
    "mdl.fit(XTtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abraham</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youth</th>\n",
       "      <th>zane</th>\n",
       "      <th>zany</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03358</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abandoned  ability     able  aboard  abraham  absence  absent  \\\n",
       "0      0.0        0.0  0.00000  0.00000     0.0      0.0      0.0     0.0   \n",
       "1      0.0        0.0  0.00000  0.04392     0.0      0.0      0.0     0.0   \n",
       "2      0.0        0.0  0.00000  0.00000     0.0      0.0      0.0     0.0   \n",
       "3      0.0        0.0  0.00000  0.00000     0.0      0.0      0.0     0.0   \n",
       "4      0.0        0.0  0.03358  0.00000     0.0      0.0      0.0     0.0   \n",
       "\n",
       "   absolute  absolutely  ...    younger  youngster  youth  zane  zany  \\\n",
       "0  0.052455         0.0  ...   0.042163        0.0    0.0   0.0   0.0   \n",
       "1  0.000000         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "2  0.000000         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "3  0.000000         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "4  0.000000         0.0  ...   0.000000        0.0    0.0   0.0   0.0   \n",
       "\n",
       "   zellweger  zero  zeta  zombie  zone  \n",
       "0        0.0   0.0   0.0     0.0   0.0  \n",
       "1        0.0   0.0   0.0     0.0   0.0  \n",
       "2        0.0   0.0   0.0     0.0   0.0  \n",
       "3        0.0   0.0   0.0     0.0   0.0  \n",
       "4        0.0   0.0   0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTtest = pd.DataFrame(tv.transform(Xtest).toarray(), columns=tv.get_feature_names())\n",
    "XTtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8425\n",
      "[[169  36]\n",
      " [ 27 168]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       205\n",
      "           1       0.82      0.86      0.84       195\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = mdl.predict(XTtest)\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Preprocess + Tfidf + W2V + Logistic Reg</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgW2V(model, df, weighted=True):\n",
    "    modeldim = model.vector_size      # test: 3\n",
    "    modelvocab = model.index2word # test: modelvocab = ['w1', 'w2', 'w3', 'w4', 'w5']\n",
    "    numdocs = (len(df))\n",
    "    avgw2v = pd.DataFrame(np.zeros((numdocs,modeldim)))\n",
    "    # for each doc \n",
    "    for i in range(numdocs):\n",
    "        # pick up only those words that have values > 0 in df\n",
    "        ser = df.loc[i][df.loc[i]>0] \n",
    "        words, scores = ser.index, ser.values\n",
    "        #  if weighted=False , set scores to 1, else pick up the real scores\n",
    "        if (weighted==False):\n",
    "            scores = scores/scores\n",
    "        # find the avg w2v doc vectors\n",
    "        sumw2v, sumweights = 0, 0\n",
    "        for s,w in zip(scores,words):\n",
    "            if w in modelvocab:\n",
    "                sumw2v += s*model[w]\n",
    "                sumweights += s\n",
    "        avgw2v.loc[i] = sumw2v/sumweights\n",
    "    return avgw2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:\n",
    "# model={'w1':np.array([1,2,3]), 'w2':np.array([10,20,30]), 'w3':np.array([100,200,300])}\n",
    "# df = pd.DataFrame([[0.1, 0.2, 0],[0,0.5,0.6]], columns=['w1','w2','w3'])\n",
    "# avgW2V(model, df, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040796</td>\n",
       "      <td>0.033139</td>\n",
       "      <td>0.051821</td>\n",
       "      <td>-0.073265</td>\n",
       "      <td>0.020475</td>\n",
       "      <td>0.068213</td>\n",
       "      <td>-0.064967</td>\n",
       "      <td>0.031043</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.938714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003768</td>\n",
       "      <td>-0.065723</td>\n",
       "      <td>-0.037015</td>\n",
       "      <td>0.095077</td>\n",
       "      <td>-0.029673</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>-0.040381</td>\n",
       "      <td>0.035788</td>\n",
       "      <td>-0.078581</td>\n",
       "      <td>0.091074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.071175</td>\n",
       "      <td>0.043001</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>-0.057242</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>-0.036498</td>\n",
       "      <td>-0.032245</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>-0.943680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>-0.053778</td>\n",
       "      <td>-0.077673</td>\n",
       "      <td>0.044421</td>\n",
       "      <td>0.027867</td>\n",
       "      <td>-0.122711</td>\n",
       "      <td>-0.035849</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>-0.006491</td>\n",
       "      <td>0.089410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059819</td>\n",
       "      <td>0.065003</td>\n",
       "      <td>-0.036036</td>\n",
       "      <td>-0.013470</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.017957</td>\n",
       "      <td>-0.067537</td>\n",
       "      <td>-0.051117</td>\n",
       "      <td>0.034497</td>\n",
       "      <td>-0.915890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044258</td>\n",
       "      <td>-0.039810</td>\n",
       "      <td>-0.016241</td>\n",
       "      <td>0.049835</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>-0.044665</td>\n",
       "      <td>0.031725</td>\n",
       "      <td>-0.033704</td>\n",
       "      <td>-0.063794</td>\n",
       "      <td>0.126043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101836</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>0.042486</td>\n",
       "      <td>-0.101465</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>-0.032198</td>\n",
       "      <td>-0.033479</td>\n",
       "      <td>-0.007490</td>\n",
       "      <td>-0.032305</td>\n",
       "      <td>-0.750848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>-0.010804</td>\n",
       "      <td>-0.035689</td>\n",
       "      <td>0.049292</td>\n",
       "      <td>-0.013687</td>\n",
       "      <td>0.111142</td>\n",
       "      <td>-0.042924</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>-0.106229</td>\n",
       "      <td>0.044582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002317</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>-0.075423</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>-0.023277</td>\n",
       "      <td>-0.047387</td>\n",
       "      <td>-0.068912</td>\n",
       "      <td>0.038276</td>\n",
       "      <td>-1.113218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>-0.053004</td>\n",
       "      <td>-0.038293</td>\n",
       "      <td>0.056650</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.118024</td>\n",
       "      <td>-0.025791</td>\n",
       "      <td>-0.033095</td>\n",
       "      <td>-0.026348</td>\n",
       "      <td>0.033411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.040796  0.033139  0.051821 -0.073265  0.020475  0.068213 -0.064967   \n",
       "1 -0.071175  0.043001  0.007508 -0.057242  0.003879  0.007345 -0.036498   \n",
       "2 -0.059819  0.065003 -0.036036 -0.013470  0.027638  0.017957 -0.067537   \n",
       "3 -0.101836  0.015897  0.042486 -0.101465  0.016956 -0.032198 -0.033479   \n",
       "4 -0.002317  0.010577  0.029671 -0.075423  0.026912 -0.023277 -0.047387   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.031043  0.020447 -0.938714    ...    -0.003768 -0.065723 -0.037015   \n",
       "1 -0.032245  0.025846 -0.943680    ...     0.019498 -0.053778 -0.077673   \n",
       "2 -0.051117  0.034497 -0.915890    ...     0.044258 -0.039810 -0.016241   \n",
       "3 -0.007490 -0.032305 -0.750848    ...     0.004617 -0.010804 -0.035689   \n",
       "4 -0.068912  0.038276 -1.113218    ...     0.019012 -0.053004 -0.038293   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.095077 -0.029673  0.027658 -0.040381  0.035788 -0.078581  0.091074  \n",
       "1  0.044421  0.027867 -0.122711 -0.035849 -0.001802 -0.006491  0.089410  \n",
       "2  0.049835  0.017009 -0.044665  0.031725 -0.033704 -0.063794  0.126043  \n",
       "3  0.049292 -0.013687  0.111142 -0.042924 -0.000535 -0.106229  0.044582  \n",
       "4  0.056650  0.020266  0.118024 -0.025791 -0.033095 -0.026348  0.033411  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw2vXTtrain = avgW2V(model, XTtrain, weighted=True)\n",
    "aw2vXTtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mdl = LogisticRegression()\n",
    "mdl.fit(aw2vXTtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.086003</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>-0.100836</td>\n",
       "      <td>0.021209</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>-0.050773</td>\n",
       "      <td>-0.042614</td>\n",
       "      <td>-0.029447</td>\n",
       "      <td>-1.053194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.051336</td>\n",
       "      <td>0.016031</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>0.024754</td>\n",
       "      <td>0.068378</td>\n",
       "      <td>-0.039660</td>\n",
       "      <td>-0.052699</td>\n",
       "      <td>-0.084385</td>\n",
       "      <td>0.060409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.052018</td>\n",
       "      <td>-0.005735</td>\n",
       "      <td>-0.017394</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.024262</td>\n",
       "      <td>0.048180</td>\n",
       "      <td>-0.084753</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.058093</td>\n",
       "      <td>-0.972033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041197</td>\n",
       "      <td>-0.065310</td>\n",
       "      <td>-0.015913</td>\n",
       "      <td>-0.043150</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>-0.104000</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>-0.062266</td>\n",
       "      <td>-0.062143</td>\n",
       "      <td>0.130106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021814</td>\n",
       "      <td>-0.030909</td>\n",
       "      <td>0.044191</td>\n",
       "      <td>-0.099178</td>\n",
       "      <td>-0.073287</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>-0.054740</td>\n",
       "      <td>0.059282</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>-1.204135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106577</td>\n",
       "      <td>-0.073240</td>\n",
       "      <td>-0.054571</td>\n",
       "      <td>0.040601</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>-0.022469</td>\n",
       "      <td>-0.073942</td>\n",
       "      <td>-0.068220</td>\n",
       "      <td>-0.069859</td>\n",
       "      <td>0.067113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.026795</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.046080</td>\n",
       "      <td>-0.087047</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.139533</td>\n",
       "      <td>-0.019929</td>\n",
       "      <td>0.075410</td>\n",
       "      <td>0.050466</td>\n",
       "      <td>-0.873131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046192</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>-0.008921</td>\n",
       "      <td>0.127608</td>\n",
       "      <td>-0.008214</td>\n",
       "      <td>0.017369</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.027751</td>\n",
       "      <td>-0.062943</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041978</td>\n",
       "      <td>0.066197</td>\n",
       "      <td>-0.004861</td>\n",
       "      <td>-0.076533</td>\n",
       "      <td>0.017936</td>\n",
       "      <td>0.084955</td>\n",
       "      <td>-0.101371</td>\n",
       "      <td>-0.055299</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>-0.895451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>-0.061260</td>\n",
       "      <td>-0.062135</td>\n",
       "      <td>0.034150</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>-0.022163</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>-0.014017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.086003  0.031486  0.024450 -0.100836  0.021209  0.032520 -0.050773   \n",
       "1 -0.052018 -0.005735 -0.017394  0.011840  0.024262  0.048180 -0.084753   \n",
       "2  0.021814 -0.030909  0.044191 -0.099178 -0.073287  0.092360 -0.054740   \n",
       "3 -0.026795  0.008634  0.046080 -0.087047  0.006421  0.139533 -0.019929   \n",
       "4 -0.041978  0.066197 -0.004861 -0.076533  0.017936  0.084955 -0.101371   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0 -0.042614 -0.029447 -1.053194    ...     0.002200 -0.051336  0.016031   \n",
       "1  0.001869  0.058093 -0.972033    ...     0.041197 -0.065310 -0.015913   \n",
       "2  0.059282  0.009402 -1.204135    ...     0.106577 -0.073240 -0.054571   \n",
       "3  0.075410  0.050466 -0.873131    ...    -0.046192 -0.011818 -0.008921   \n",
       "4 -0.055299  0.055869 -0.895451    ...     0.058752 -0.061260 -0.062135   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.041343  0.024754  0.068378 -0.039660 -0.052699 -0.084385  0.060409  \n",
       "1 -0.043150  0.007570 -0.104000  0.016701 -0.062266 -0.062143  0.130106  \n",
       "2  0.040601  0.007646 -0.022469 -0.073942 -0.068220 -0.069859  0.067113  \n",
       "3  0.127608 -0.008214  0.017369  0.012958  0.027751 -0.062943  0.000473  \n",
       "4  0.034150  0.001016  0.007925 -0.022163  0.007811 -0.051953 -0.014017  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw2vXTtest = avgW2V(model, XTtest, weighted=True)\n",
    "aw2vXTtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "[[160  45]\n",
      " [ 35 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       205\n",
      "           1       0.78      0.82      0.80       195\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = mdl.predict(aw2vXTtest)\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
