{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) read and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e38f87ecf4e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = spark.read.csv('train.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# num rows and num cols\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top 20 rows\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|\n",
      "+--------+------+----+-----+-----+-------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|\n",
      "|       1|     1|38.0|    1|    0|71.2833|\n",
      "|       1|     3|26.0|    0|    0|  7.925|\n",
      "|       1|     1|35.0|    1|    0|   53.1|\n",
      "|       0|     3|35.0|    0|    0|   8.05|\n",
      "|       0|     3|null|    0|    0| 8.4583|\n",
      "|       0|     1|54.0|    0|    0|51.8625|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|\n",
      "|       1|     3|27.0|    0|    2|11.1333|\n",
      "|       1|     2|14.0|    1|    0|30.0708|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|\n",
      "|       1|     1|58.0|    0|    0|  26.55|\n",
      "|       0|     3|20.0|    0|    0|   8.05|\n",
      "|       0|     3|39.0|    1|    5| 31.275|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|\n",
      "|       1|     2|55.0|    0|    0|   16.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|\n",
      "|       1|     2|null|    0|    0|   13.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|\n",
      "|       1|     3|null|    0|    0|  7.225|\n",
      "+--------+------+----+-----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns\n",
    "df = df.drop('PassengerId','Name','Ticket','Cabin','Embarked','Sex')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method-1: Non-Pipeline Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "dftrain, dftest = df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) fit and transform training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|          Survived|            Pclass|               Age|             SibSp|            Parch|             Fare|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|               725|               725|               572|               725|              725|              725|\n",
      "|   mean|0.3917241379310345| 2.326896551724138|29.217954545454543|0.5296551724137931|0.383448275862069|31.68006220689653|\n",
      "| stddev|0.4884725669440271|0.8289692038556871|14.127279327699085|1.1091111952192345|0.817117778007452|47.76285732711233|\n",
      "|    min|                 0|                 1|              0.42|                 0|                0|              0.0|\n",
      "|    max|                 1|                 3|              74.0|                 8|                6|         512.3292|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descriptive stats - note null vals (based on counts)\n",
    "dftrain.describe().show() # df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| Age|Pclass|\n",
      "+----+------+\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "|null|     1|\n",
      "| 2.0|     1|\n",
      "|18.0|     1|\n",
      "|19.0|     1|\n",
      "|19.0|     1|\n",
      "|21.0|     1|\n",
      "|24.0|     1|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select columns\n",
    "dftrain.select('Age','Pclass').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  169|\n",
      "|     3|  406|\n",
      "|     2|  150|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use groupby to get at the Pclass values\n",
    "dftrain.groupby('Pclass').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+--------+------+\n",
      "|Survived|Pclass|SibSp|Parch|    Fare|impAge|\n",
      "+--------+------+-----+-----+--------+------+\n",
      "|       0|     1|    0|    0|     0.0|  28.0|\n",
      "|       0|     1|    0|    0|     0.0|  28.0|\n",
      "|       0|     1|    0|    0|  25.925|  28.0|\n",
      "|       0|     1|    0|    0|    26.0|  28.0|\n",
      "|       0|     1|    0|    0| 27.7208|  28.0|\n",
      "|       0|     1|    0|    0| 27.7208|  28.0|\n",
      "|       0|     1|    0|    0| 30.6958|  28.0|\n",
      "|       0|     1|    0|    0|    35.0|  28.0|\n",
      "|       0|     1|    0|    0|    39.6|  28.0|\n",
      "|       0|     1|    0|    0|    42.4|  28.0|\n",
      "|       0|     1|    0|    0|    50.0|  28.0|\n",
      "|       0|     1|    0|    0|    52.0|  28.0|\n",
      "|       0|     1|    0|    0|221.7792|  28.0|\n",
      "|       0|     1|    0|    0| 227.525|  28.0|\n",
      "|       0|     1|    1|    2|  151.55|   2.0|\n",
      "|       0|     1|    1|    0|   108.9|  18.0|\n",
      "|       0|     1|    1|    0|    53.1|  19.0|\n",
      "|       0|     1|    3|    2|   263.0|  19.0|\n",
      "|       0|     1|    0|    1| 77.2875|  21.0|\n",
      "|       0|     1|    0|    0|    79.2|  24.0|\n",
      "+--------+------+-----+-----+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# impute Age\n",
    "from pyspark.ml.feature import Imputer\n",
    "impage = Imputer(strategy='median', inputCols=['Age'],outputCols=['impAge']).fit(dftrain)\n",
    "dftrain = impage.transform(dftrain)\n",
    "dftrain = dftrain.drop('Age')\n",
    "dftrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+--------+------+-------------+\n",
      "|Survived|SibSp|Parch|    Fare|impAge|    ohePclass|\n",
      "+--------+-----+-----+--------+------+-------------+\n",
      "|       0|    0|    0|     0.0|  28.0|(4,[1],[1.0])|\n",
      "|       0|    0|    1|247.5208|  24.0|(4,[1],[1.0])|\n",
      "|       0|    1|    0|    52.0|  42.0|(4,[1],[1.0])|\n",
      "|       0|    0|    0|    28.5|  45.5|(4,[1],[1.0])|\n",
      "|       0|    0|    0| 28.7125|  50.0|(4,[1],[1.0])|\n",
      "|       0|    1|    0| 106.425|  50.0|(4,[1],[1.0])|\n",
      "|       0|    0|    0|   26.55|  56.0|(4,[1],[1.0])|\n",
      "|       0|    0|    0|    11.5|  18.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    73.5|  18.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    10.5|  23.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    10.5|  24.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    13.0|  25.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    26.0|  27.0|(4,[2],[1.0])|\n",
      "|       0|    1|    0|    21.0|  27.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    10.5|  31.0|(4,[2],[1.0])|\n",
      "|       0|    1|    0| 30.0708|  32.5|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    13.0|  39.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|    26.0|  39.0|(4,[2],[1.0])|\n",
      "|       0|    0|    0|  7.2292|  28.0|(4,[0],[1.0])|\n",
      "|       0|    0|    0|  7.2292|  28.0|(4,[0],[1.0])|\n",
      "+--------+-----+-----+--------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encode Pclass\n",
    "\n",
    "# categorical values must first be converted to category indices\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "sipclass = StringIndexer(handleInvalid='keep', \n",
    "                         inputCol='Pclass', outputCol='idxPclass').fit(dftrain)\n",
    "dftrain = sipclass.transform(dftrain)\n",
    "dftrain = dftrain.drop('Pclass')\n",
    "\n",
    "# category indices can then be converted to OHE\n",
    "# note: output represented in sparse format - so 0s not printed:\n",
    "# so for instance, (3,[2],[1.0]) means OHE vector of length 3 with a 1.0 at position 2, and 0 elsewhere.\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "ohe = OneHotEncoderEstimator(handleInvalid='keep', dropLast=True,\n",
    "                             inputCols=['idxPclass'], \n",
    "                             outputCols=['ohePclass']).fit(dftrain)\n",
    "dftrain = ohe.transform(dftrain)\n",
    "dftrain = dftrain.drop('idxPclass')\n",
    "dftrain.sample(withReplacement=False, fraction=0.1).limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|            features|\n",
      "+--------+--------------------+\n",
      "|       0|(8,[3,5],[28.0,1.0])|\n",
      "|       0|(8,[3,5],[28.0,1.0])|\n",
      "|       0|(8,[2,3,5],[25.92...|\n",
      "|       0|(8,[2,3,5],[26.0,...|\n",
      "|       0|(8,[2,3,5],[27.72...|\n",
      "|       0|(8,[2,3,5],[27.72...|\n",
      "|       0|(8,[2,3,5],[30.69...|\n",
      "|       0|(8,[2,3,5],[35.0,...|\n",
      "|       0|(8,[2,3,5],[39.6,...|\n",
      "|       0|(8,[2,3,5],[42.4,...|\n",
      "|       0|(8,[2,3,5],[50.0,...|\n",
      "|       0|(8,[2,3,5],[52.0,...|\n",
      "|       0|(8,[2,3,5],[221.7...|\n",
      "|       0|(8,[2,3,5],[227.5...|\n",
      "|       0|[1.0,2.0,151.55,2...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|[3.0,2.0,263.0,19...|\n",
      "|       0|(8,[1,2,3,5],[1.0...|\n",
      "|       0|(8,[2,3,5],[79.2,...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine all feature columns into vector form to feed into ML model\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "va = VectorAssembler(inputCols=['SibSp','Parch','Fare','impAge','ohePclass'],outputCol='features')\n",
    "dftrain = va.transform(dftrain) # \n",
    "dftrain = dftrain.drop('SibSp','Parch','Fare','impAge','ohePclass')\n",
    "dftrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train random forest classifier model on training data\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\",\n",
    "                            numTrees=100).fit(dftrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c) tranform, predict and evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|    Fare|\n",
      "+--------+------+----+-----+-----+--------+\n",
      "|       0|     1|null|    0|    0|   26.55|\n",
      "|       0|     1|null|    0|    0|    31.0|\n",
      "|       0|     1|22.0|    0|    0|135.6333|\n",
      "|       0|     1|25.0|    1|    2|  151.55|\n",
      "|       0|     1|27.0|    0|    2|   211.5|\n",
      "|       0|     1|30.0|    0|    0|   27.75|\n",
      "|       0|     1|36.0|    1|    0|   78.85|\n",
      "|       0|     1|37.0|    1|    0|    53.1|\n",
      "|       0|     1|38.0|    0|    0|     0.0|\n",
      "|       0|     1|40.0|    0|    0|     0.0|\n",
      "|       0|     1|40.0|    0|    0| 27.7208|\n",
      "|       0|     1|45.0|    0|    0|    35.5|\n",
      "|       0|     1|46.0|    1|    0|  61.175|\n",
      "|       0|     1|50.0|    1|    0|    55.9|\n",
      "|       0|     1|56.0|    0|    0| 30.6958|\n",
      "|       0|     1|58.0|    0|    0|    29.7|\n",
      "|       0|     1|58.0|    0|    2| 113.275|\n",
      "|       0|     1|60.0|    0|    0|   26.55|\n",
      "|       0|     1|64.0|    0|    0|    26.0|\n",
      "|       0|     1|65.0|    0|    1| 61.9792|\n",
      "+--------+------+----+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|            features|\n",
      "+--------+--------------------+\n",
      "|       0|(8,[2,3,5],[26.55...|\n",
      "|       0|(8,[2,3,5],[31.0,...|\n",
      "|       0|(8,[2,3,5],[135.6...|\n",
      "|       0|[1.0,2.0,151.55,2...|\n",
      "|       0|(8,[1,2,3,5],[2.0...|\n",
      "|       0|(8,[2,3,5],[27.75...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|(8,[3,5],[38.0,1.0])|\n",
      "|       0|(8,[3,5],[40.0,1.0])|\n",
      "|       0|(8,[2,3,5],[27.72...|\n",
      "|       0|(8,[2,3,5],[35.5,...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|(8,[0,2,3,5],[1.0...|\n",
      "|       0|(8,[2,3,5],[30.69...|\n",
      "|       0|(8,[2,3,5],[29.7,...|\n",
      "|       0|(8,[1,2,3,5],[2.0...|\n",
      "|       0|(8,[2,3,5],[26.55...|\n",
      "|       0|(8,[2,3,5],[26.0,...|\n",
      "|       0|(8,[1,2,3,5],[1.0...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# impute Age\n",
    "dftest = impage.transform(dftest)\n",
    "dftest = dftest.drop('Age')\n",
    "\n",
    "# si and ohe Pclass, Sex, Embarked\n",
    "dftest = sipclass.transform(dftest)\n",
    "dftest = dftest.drop('Pclass')\n",
    "dftest = ohe.transform(dftest)\n",
    "dftest = dftest.drop('idxPclass')\n",
    "\n",
    "# combine all feature columns into vector form to feed into ML model\n",
    "dftest = va.transform(dftest)\n",
    "dftest = dftest.drop('SibSp','Parch','Fare','impAge','ohePclass')\n",
    "dftest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|(8,[2,3,5],[26.55...|[45.3504588863735...|[0.45350458886373...|       1.0|\n",
      "|       0|(8,[2,3,5],[31.0,...|[44.4172924206899...|[0.44417292420689...|       1.0|\n",
      "|       0|(8,[2,3,5],[135.6...|[20.2622153518000...|[0.20262215351800...|       1.0|\n",
      "|       0|[1.0,2.0,151.55,2...|[23.1149116045624...|[0.23114911604562...|       1.0|\n",
      "|       0|(8,[1,2,3,5],[2.0...|[23.7277946209613...|[0.23727794620961...|       1.0|\n",
      "|       0|(8,[2,3,5],[27.75...|[45.1903519440570...|[0.45190351944057...|       1.0|\n",
      "|       0|(8,[0,2,3,5],[1.0...|[21.3991127327763...|[0.21399112732776...|       1.0|\n",
      "|       0|(8,[0,2,3,5],[1.0...|[33.0898737409890...|[0.33089873740989...|       1.0|\n",
      "|       0|(8,[3,5],[38.0,1.0])|[70.3179246693724...|[0.70317924669372...|       0.0|\n",
      "|       0|(8,[3,5],[40.0,1.0])|[70.3179246693724...|[0.70317924669372...|       0.0|\n",
      "|       0|(8,[2,3,5],[27.72...|[46.2727347774044...|[0.46272734777404...|       1.0|\n",
      "|       0|(8,[2,3,5],[35.5,...|[53.1563915398116...|[0.53156391539811...|       0.0|\n",
      "|       0|(8,[0,2,3,5],[1.0...|[26.7524667506650...|[0.26752466750665...|       1.0|\n",
      "|       0|(8,[0,2,3,5],[1.0...|[23.7926509214993...|[0.23792650921499...|       1.0|\n",
      "|       0|(8,[2,3,5],[30.69...|[55.6092527175687...|[0.55609252717568...|       0.0|\n",
      "|       0|(8,[2,3,5],[29.7,...|[60.7196189711605...|[0.60719618971160...|       0.0|\n",
      "|       0|(8,[1,2,3,5],[2.0...|[32.4207730278424...|[0.32420773027842...|       1.0|\n",
      "|       0|(8,[2,3,5],[26.55...|[62.3971267277370...|[0.62397126727737...|       0.0|\n",
      "|       0|(8,[2,3,5],[26.0,...|[61.6447924955566...|[0.61644792495556...|       0.0|\n",
      "|       0|(8,[1,2,3,5],[1.0...|[43.3485048377590...|[0.43348504837759...|       1.0|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict using random forest classifier on test data\n",
    "predictions = rfc.transform(dftest)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7349397590361446"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate prediction results\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method-2: Pipeline Solution (instead of Method-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a) split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "dftrain, dftest = df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) build pipeline and tuse it to fit/tranform on train and transform/predict/evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7349397590361446"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up age imputation\n",
    "impage = Imputer(strategy='median', inputCols=['Age'],outputCols=['impAge'])\n",
    "\n",
    "# set up Pclass indexing\n",
    "sipclass = StringIndexer(handleInvalid='keep', \n",
    "                        inputCol='Pclass', outputCol='idxPclass')\n",
    "\n",
    "# set up Pclass OHE\n",
    "ohe = OneHotEncoderEstimator(handleInvalid='keep', dropLast=True,\n",
    "                             inputCols=['idxPclass'], \n",
    "                             outputCols=['ohePclass'])\n",
    "\n",
    "# set up feature vector assembly\n",
    "va = VectorAssembler(inputCols=['SibSp','Parch','Fare','impAge','ohePclass'],outputCol='features')\n",
    "\n",
    "# set up classification model\n",
    "rfc = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# create pipeline with proprocessing and classification steps\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[impage, sipclass, ohe, va, rfc]) \n",
    "\n",
    "# train model with training data\n",
    "mdl = pipeline.fit(dftrain)\n",
    "\n",
    "# predict on test data\n",
    "pred = mdl.transform(dftest)\n",
    "\n",
    "# evaluate predictions\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
